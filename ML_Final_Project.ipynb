{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "F5a5xkPVS3Vj",
        "aMb-4QhcS7EN",
        "l7pDDsa1Sufr",
        "6nrei10hSqBz",
        "-QqT_tm7TQhF",
        "fC3tWq41TU32",
        "Qj2J7i2PThtz",
        "zGeEeDd6TtVf",
        "92i93RoRH8eT",
        "hDIUzYb3bR5i",
        "8l_yOo23VJkC",
        "lDy-n8O3X_vB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install Deepface"
      ],
      "metadata": {
        "id": "F5a5xkPVS3Vj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xD2Nvx45SGYf"
      },
      "outputs": [],
      "source": [
        "# Need to install deepface if not already installed in the notebook\n",
        "!pip install deepface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Connect Drive"
      ],
      "metadata": {
        "id": "aMb-4QhcS7EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  # Module to interact with Google Drive in Google Colab\n",
        "drive.mount('/content/drive')  # Mount Google Drive to access files stored in the drive"
      ],
      "metadata": {
        "id": "3ykJAt77SSvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries"
      ],
      "metadata": {
        "id": "l7pDDsa1Sufr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2  # OpenCV library for image and video processing\n",
        "import os  # Provides functions to interact with the operating system\n",
        "import matplotlib.pyplot as plt  # Library for creating visualizations and plots\n",
        "import pandas as pd  # Data manipulation and analysis library\n",
        "import pickle  # For serializing and deserializing Python object structures\n",
        "import joblib  # Library for saving and loading machine learning models\n",
        "import shutil  # High-level file operations like copying and removal\n",
        "import numpy as np  # Library for numerical computations and array operations\n",
        "\n",
        "from sklearn.svm import SVC  # Support Vector Classification\n",
        "from copy import deepcopy  # Create deep copies of objects\n",
        "from sklearn.model_selection import train_test_split  # Split arrays or matrices into random train and test subsets\n",
        "from sklearn.metrics import accuracy_score  # For calculating accuracy classification score\n",
        "from sklearn.preprocessing import StandardScaler  # For standardizing features by removing the mean and scaling to unit variance\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # Linear Discriminant Analysis\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img  # Utilities for image preprocessing and augmentation\n",
        "from skimage import exposure  # For histogram equalization and other exposure adjustment techniques\n",
        "from deepface import DeepFace  # Deep learning library for face recognition and facial attribute analysis\n",
        "from sklearn.pipeline import Pipeline  # Pipeline utility for simplifying the creation of machine learning workflows\n",
        "from sklearn.decomposition import PCA  # Principal Component Analysis for dimensionality reduction"
      ],
      "metadata": {
        "id": "C40VN1MPSSsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image Augmentation"
      ],
      "metadata": {
        "id": "6nrei10hSqBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define root directory and subdirectories\n",
        "# root_dir = \"/content/drive/MyDrive/SP24-CSEN240-2/Project/Train\"\n",
        "# data_dir = root_dir\n",
        "# #data_dir = root_dir + \"/Training\"\n",
        "# label_file = data_dir + \"/labels.txt\"\n",
        "# output_dir = root_dir + \"/Augmented_images\"\n",
        "\n",
        "# # Load label mappings from a space-separated text file\n",
        "# label_mapping = {}\n",
        "# with open(label_file, 'r') as file:\n",
        "#     for line in file:\n",
        "#         # Read filename and label from each line, and store in dictionary\n",
        "#         filename, label = line.strip().split(' ')\n",
        "#         label_mapping[filename + \".jpg\"] = label\n",
        "# print(label_mapping)\n",
        "\n",
        "# # Initialize the ImageDataGenerator with desired augmentations\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=5,\n",
        "#     rescale=1./255,\n",
        "#     zoom_range=0.2,\n",
        "#     brightness_range=(0.5, 1.0),\n",
        "#     horizontal_flip=True,\n",
        "#     fill_mode='nearest'\n",
        "# )\n",
        "\n",
        "# # Function to save augmented images in label-specific folders\n",
        "# def save_augmented_images(directory, output_directory, label_mapping, num_augmented_images=5):\n",
        "#     for filename in os.listdir(directory):\n",
        "#         if filename in label_mapping:  # Check if the file has a mapping\n",
        "#             label = label_mapping[filename]  # Get the label for the current file\n",
        "#             label_dir = os.path.join(output_directory, label)  # Define label-specific directory path\n",
        "\n",
        "#             if not os.path.exists(label_dir):\n",
        "#                 os.makedirs(label_dir)  # Create the directory if it doesn't exist\n",
        "\n",
        "#             file_path = os.path.join(directory, filename)\n",
        "#             image = load_img(file_path)  # Load the image\n",
        "#             image_array = img_to_array(image)  # Convert image to array\n",
        "#             image_array = image_array.reshape((1,) + image_array.shape)\n",
        "\n",
        "#             # Generate and save augmented images\n",
        "#             i = 0\n",
        "#             save_prefix = os.path.splitext(filename)[0]  # Use filename without extension as save prefix\n",
        "#             for batch in datagen.flow(image_array, batch_size=1, save_to_dir=label_dir, save_prefix=save_prefix, save_format='jpeg'):\n",
        "#                 i += 1\n",
        "#                 if i >= num_augmented_images:\n",
        "#                     break  # Limit the number of augmented images generated per original image"
      ],
      "metadata": {
        "id": "2DBacPCySSpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to start the augmentation process\n",
        "#save_augmented_images(data_dir, output_dir, label_mapping)"
      ],
      "metadata": {
        "id": "k25M_B5pSSmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Cleaning"
      ],
      "metadata": {
        "id": "-QqT_tm7TQhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect faces in an image\n",
        "def detect_faces(img: np.ndarray) -> np.ndarray:\n",
        "    try:\n",
        "        # Use DeepFace to extract faces, returning the first detected face\n",
        "        img = DeepFace.extract_faces(img)[0]['face']\n",
        "        return img\n",
        "    except ValueError:\n",
        "        # If no face is detected, return an empty array\n",
        "        return np.empty(shape=(0,))"
      ],
      "metadata": {
        "id": "JmMDTbzLSSjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load images from folders"
      ],
      "metadata": {
        "id": "fC3tWq41TU32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder: str, file_map: map) -> list:\n",
        "    print(\"loading images from folder\")\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        if os.path.isfile(img_path):\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                images.append(img)\n",
        "                labels.append(file_map[filename])\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "def preprocess_images(images: list, face_detection: bool) -> list:\n",
        "    processed_images = []\n",
        "    print(\"pre - processing images\")\n",
        "    wrongCrop = 0\n",
        "    for i,img in enumerate(images):\n",
        "        backup = deepcopy(img)\n",
        "        # img = cv2.resize(img, (200, 200))\n",
        "        if face_detection == True:\n",
        "            img = detect_faces(img)\n",
        "            if img.size == 0:  # Check if the image is empty after face cropping\n",
        "                wrongCrop += 1\n",
        "                img = backup\n",
        "        img = cv2.resize(img, (128, 128))\n",
        "        processed_images.append(img)\n",
        "    return processed_images\n",
        "\n",
        "def load_labels_from_file(mapping_file):\n",
        "    label_map = {}\n",
        "    with open(mapping_file, 'r') as file:\n",
        "        for line in file:\n",
        "            filename, label = line.strip().split(' ')\n",
        "            label_map[filename + \".jpg\"] = label\n",
        "    return label_map\n",
        "\n",
        "def load_keras_images(path):\n",
        "    folders = os.listdir(path)\n",
        "    labels = []\n",
        "    images = []\n",
        "    for name in folders:\n",
        "        folder_path = os.path.join(path,name)\n",
        "        img_path = os.listdir(folder_path)\n",
        "        for img in img_path:\n",
        "            image = cv2.imread(os.path.join(folder_path, img))\n",
        "            images.append(image)\n",
        "            labels.append(name)\n",
        "    return images, labels\n",
        "\n",
        "def load_images(path:str, Face_detect = False, keras = False) -> np.array:\n",
        "    # Load images from a folder\n",
        "    images = []\n",
        "    if not keras:\n",
        "        # Load labels from the mapping file\n",
        "        mapping_file_path = os.path.join(path,\"labels.txt\")\n",
        "        label_map = load_labels_from_file(mapping_file_path)\n",
        "        images, labels = load_images_from_folder(path, label_map)\n",
        "    else:\n",
        "        images, labels = load_keras_images(path)\n",
        "    # Preprocess the images\n",
        "    preprocessed_images = preprocess_images(images, Face_detect)\n",
        "    # Convert preprocessed images to numpy array\n",
        "    data = np.array(preprocessed_images)\n",
        "    # Reshape data to flatten\n",
        "    data = data.reshape(len(data),-1)\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "81XiMbXdSSgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Data"
      ],
      "metadata": {
        "id": "Qj2J7i2PThtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set flag for face detection\n",
        "Face_detect = True\n",
        "\n",
        "# Define the root path for the project\n",
        "root_path = \"/content/drive/MyDrive/SP24-CSEN240-2/Project/Temp/Group-12\"\n",
        "\n",
        "# Define the path for training images\n",
        "train_path = root_path + \"/Augmented_images\"\n",
        "\n",
        "# Define the path for validation images\n",
        "#validation_path = root_path + \"/Validation\"\n",
        "\n",
        "# Load training images and labels, with face detection enabled and Keras format\n",
        "data, labels = load_images(train_path, Face_detect, keras=True)\n",
        "\n",
        "# Load validation images and labels, with face detection enabled\n",
        "#validation_data, validation_labels = load_images(validation_path, Face_detect)"
      ],
      "metadata": {
        "id": "LC4six3DSSdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save the Data and Validation file for easy read and write."
      ],
      "metadata": {
        "id": "zGeEeDd6TtVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save Data\n",
        "# root_path = \"/content/drive/MyDrive/ML_Project\"\n",
        "\n",
        "# # Save training data as a .npy file\n",
        "# np.save(root_path + '/Training_data.npy', data)\n",
        "\n",
        "# # Save validation data as a .npy file\n",
        "# np.save(root_path + '/Validation_data.npy', validation_data)\n",
        "\n",
        "# # Save training data labels as a .pkl file\n",
        "# with open(root_path + '/Training_data_labels.pkl', 'wb') as f:\n",
        "#     pickle.dump(labels, f)\n",
        "\n",
        "# # Save validation data labels as a .pkl file\n",
        "# with open(root_path + '/Validation_data_labels.pkl', 'wb') as f:\n",
        "#     pickle.dump(validation_labels, f)"
      ],
      "metadata": {
        "id": "rLcjkI82nFJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load Data\n",
        "# # Load training and testing data from .npy files\n",
        "# train_test_data_load = np.load(root_path + '/Training_data.npy')\n",
        "\n",
        "# # Load validation data from .npy file\n",
        "# validation_data_load = np.load(root_path + '/Validation_data.npy')\n",
        "\n",
        "# # Load training and testing labels from .pkl file\n",
        "# with open(root_path + '/Training_data_labels.pkl', 'rb') as f:\n",
        "#     train_test_labels_load = pickle.load(f)\n",
        "\n",
        "# # Load validation labels from .pkl file\n",
        "# with open(root_path + '/Validation_data_labels.pkl', 'rb') as f:\n",
        "#     validation_labels_load = pickle.load(f)"
      ],
      "metadata": {
        "id": "Qv3YM3VdnJDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ML Model"
      ],
      "metadata": {
        "id": "92i93RoRH8eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the pipeline\n",
        "pip = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Apply standard scaling to the data\n",
        "    ('pca', PCA(n_components=600)),  # Perform PCA with 600 components\n",
        "    ('lda', LinearDiscriminantAnalysis(n_components=34)),  # Perform LDA with 34 components\n",
        "    ('svm', SVC(C=32))  # Support Vector Classification with C=32\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pip.fit(data_train, labels_train)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "#score = pip.score(validation_data, validation_labels)\n",
        "\n",
        "# Print the score\n",
        "#print(score)"
      ],
      "metadata": {
        "id": "9JUcCir4SSat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the score of the model using the test data and labels\n",
        "score = pip.score(data_test, labels_test)\n",
        "\n",
        "# Print the score\n",
        "print(score)"
      ],
      "metadata": {
        "id": "cGAIRC3XJsTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save and Load Model"
      ],
      "metadata": {
        "id": "hDIUzYb3bR5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the trained pipeline\n",
        "# joblib.dump(pip, '/content/drive/MyDrive/ML_Project/trained_model.pkl')"
      ],
      "metadata": {
        "id": "5krJikwpbTPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the trained pipeline\n",
        "# Fr_model = joblib.load('/content/drive/MyDrive/ML_Project/trained_model.pkl')"
      ],
      "metadata": {
        "id": "Bqymxec7bVjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict"
      ],
      "metadata": {
        "id": "8l_yOo23VJkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run the cell if there is no labels file.\n",
        "\n",
        "# # Define the folder path\n",
        "# folder_path = \"/content/drive/MyDrive/SP24-CSEN240-2/Project/Test\"\n",
        "\n",
        "# # Initialize an empty list to store file names\n",
        "# file_list = []\n",
        "\n",
        "# # Iterate through all files in the folder\n",
        "# for filename in os.listdir(folder_path):\n",
        "#     # Add filename and \"None\" to the list as a tab-separated string\n",
        "#     file_list.append(os.path.splitext(filename)[0] + \" none\")\n",
        "\n",
        "# # Define the output file path\n",
        "# output_file = \"/content/drive/MyDrive/SP24-CSEN240-2/Project/Test/labels.txt\"\n",
        "\n",
        "# # Write the file names and \"None\" to the output file\n",
        "# with open(output_file, 'w') as f:\n",
        "#     f.write(\"\\n\".join(file_list))"
      ],
      "metadata": {
        "id": "r5jCaKjqV9fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the folder containing the test images\n",
        "prediction_path = \"/content/drive/MyDrive/SP24-CSEN240-2/Project/Test\"\n",
        "\n",
        "# Load the test images and their labels\n",
        "prediction_data, prediction_labels = load_images(prediction_path, Face_detect)"
      ],
      "metadata": {
        "id": "UlrIzR3pVJOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the validation data\n",
        "predictions = pip.predict(prediction_data)"
      ],
      "metadata": {
        "id": "eIgMC8WOV-va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Output the predictions"
      ],
      "metadata": {
        "id": "lDy-n8O3X_vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output file path\n",
        "output_file = \"/content/drive/MyDrive/SP24-CSEN240-2/Project/Prediction/group-12.txt\"\n",
        "# Write the array to the output file\n",
        "with open(output_file, 'w') as f:\n",
        "    for name in predictions:\n",
        "        f.write(name + \"\\n\")\n",
        "print(\"Array saved to:\", output_file)"
      ],
      "metadata": {
        "id": "7j1YwVadXm4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#END"
      ],
      "metadata": {
        "id": "kxQ8p3qOYRiG"
      }
    }
  ]
}